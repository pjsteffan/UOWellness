{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\n",
      "2024-11-19 03:10:25.714676\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a hash from text\n",
    "def generate_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "\n",
    "t = \"Hello World\"\n",
    "a = generate_hash(t)\n",
    "print(t)\n",
    "print(a)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame({\n",
    "    'text': [t],\n",
    "    'hash': [a],\n",
    "    'date': [datetime.now()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = DF['hash'] == a\n",
    "df = DF[mask]\n",
    "\n",
    "df_not = DF[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_pickle('/app/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, URLs, output_dir='/app/BFS', max_depth=4,frame=None):\n",
    "        self.URLs = URLs\n",
    "        self.max_depth = max_depth\n",
    "        self.visited_urls = set()\n",
    "        self.frame = frame\n",
    "        self.output_dir = output_dir\n",
    "        self.check_dataframe()\n",
    "\n",
    "    # Function to generate a hash from text\n",
    "    def generate_hash(self,text):\n",
    "        return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "    def check_dataframe(self):\n",
    "        if self.frame is None:\n",
    "            self.frame = pd.DataFrame(columns=['url', 'text_hash','date_accessed'])\n",
    "        else:\n",
    "            self.frame = pd.read_pickle(self.frame)\n",
    "\n",
    "    def save_dataframe(self):\n",
    "        self.frame.to_pickle(os.path.join(self.output_dir,'dataframe.pkl'))\n",
    "\n",
    "    def url_to_filename(self,url):\n",
    "        filename = re.sub(r'^(http|https)://', '', url)\n",
    "        filename = filename.replace('/', '_')\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_]', '_', filename)\n",
    "        max_length = 255\n",
    "        return filename[:max_length]\n",
    "\n",
    "\n",
    "    def reset_visited_urls(self):\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    def scrape_text_bfs(self,start_url, base_url, max_depth=4):\n",
    "        self.reset_visited_urls()\n",
    "        \n",
    "        queue = deque([(start_url, 0)])  # Queue stores tuples of (url, current_depth)\n",
    "\n",
    "        while queue and len(self.visited_urls) < 3:\n",
    "            url, depth = queue.popleft()\n",
    "            print(f\"Scraping {url} at depth {depth}\")\n",
    "            \n",
    "            # Check if the URL has already been visited or if it exceeds max depth\n",
    "            if url in self.visited_urls or depth > max_depth:\n",
    "                continue\n",
    "            \n",
    "            mask = self.frame['url'] == url\n",
    "            entry = self.frame[mask]\n",
    "            \n",
    "            try:\n",
    "                date_accessed = entry['date_accessed'].values[0]\n",
    "            except:\n",
    "                date_accessed = None\n",
    "\n",
    "            # Mark the URL as visited\n",
    "            self.visited_urls.add(url)\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to retrieve {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if (mask.sum() == 1 and date_accessed == pd.Timestamp.now()-pd.Timedelta(weeks=1)) or mask.sum() == 0:\n",
    "                # Extract and save text\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "                text = f\"URL:{url}\\n{text}\"\n",
    "\n",
    "                try:\n",
    "                    text_hash = entry['text_hash'].values[0]\n",
    "                except:\n",
    "                    text_hash = None\n",
    "\n",
    "                file_path = os.path.join(self.output_dir, self.url_to_filename(url) + '.txt')\n",
    "\n",
    "                #See if the stored hash is the same as the hash for current text\n",
    "                if text_hash == self.generate_hash(text):\n",
    "                    pass\n",
    "                elif mask.sum() == 0:\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "                \n",
    "                elif mask.sum() == 1:\n",
    "                    #Delete the corresponding file\n",
    "                    os.remove(entry['file_path'].values[0])\n",
    "                    \n",
    "                    #Delete the entry in the dataframe\n",
    "                    self.frame = self.frame[~mask]\n",
    "\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "            \n",
    "            \n",
    "            # If the current depth is less than max_depth, find and add links to the queue\n",
    "            if depth < max_depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link['href']\n",
    "                    next_url = urljoin(base_url, href)\n",
    "                    if urlparse(next_url).netloc == urlparse(base_url).netloc and next_url not in self.visited_urls:\n",
    "                        queue.append((next_url, depth + 1))\n",
    "                        time.sleep(0.5)  # Sleep for 500ms to avoid hammering the server\n",
    "        self.save_dataframe()\n",
    "\n",
    "    def breadth_scrape(self):\n",
    "        for start_url in tqdm(self.URLs):\n",
    "            self.scrape_text_bfs(start_url, start_url, max_depth=self.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = Scraper(['https://health.uoregon.edu/'],'/app/obj_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://health.uoregon.edu/ at depth 0\n",
      "Scraping https://health.uoregon.edu/#main-content at depth 1\n",
      "Scraping https://health.uoregon.edu/search at depth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:31<00:00, 631.17s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape.breadth_scrape()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
